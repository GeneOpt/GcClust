---
title: "User's guide for package GcClust"
author: "Karl J. Ellefsen and David B. Smith"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{User Guide}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# Introduction
This user's guide describes how package GcClust is used to cluster 
regional geochemical data. To this end, this guide describes how to use
the functions in package GcClust and how to evaluate the results returned by
those functions. 

The scope of this user's guided is limited. It does not provide detailed descriptions of the function because this information is available in the package Help. It does not describe the mathematics of the finite mixture model; this description will be published separately. It does not describe Bayesian data analysis including Monte Carlo sampling, methods for analyzing compositional data such as geochemical data, and geochemical interpretation of 
clusters. We assumed that you are familar with these topics. If not, you may find information about Bayesian data analysis in  Gelman and others (2014) and information about compostional data analysis in Pawlowsky-Glahn and others (2015).

The goals of this user's guide are most readily achieved by showing the 
step-by-step processing of a data set, so this package includes a 
regional geochemical data set.
The data set comprises measurements of 959 soil samples that were collected in 
the state of Colorado, United States of America. For each sample, 
the measurements
include the chemical concentrations of 39 elements. Additional information
about these data are listed under "CoGeochemData" in the package Help.

This user's guide includes R-language scripts that are used to cluster the
Colorado data. We highly recommend that you copy these scripts into a
file and execute the scripts. This effort will
help you become familar with the processing and its analysis. Some details 
of your
processing results will differ from the results presented in this
user's guide because of the random nature of the calculations. Nonetheless, 
the general features of your processing results will be the same.

The remainder of the user's guide is divided into 8 sections:
[Organization of the geochemical data],
[Preparatory steps],
[Data transformation and analysis], 
[Sampling the posterior probability density function],
[Analysis and processing of the Monte Carlo samples],
[Cluster visualization], "Splitting the data" and [References].

# Organization of the geochemical data

## First element

The geochemical data is organized as a R-language list with three elements. The first list element is called "concData", and it contains the geochemical concentrations and related information. The list element is a data container, 
which is called a `SpatialPointsDataFrame`. This container is defined in the
"sp" package and was designed specifically for spatial data. 
A SpatialPointsDataFrame comprises several sub-containers, which are fully described in the sp package documentation. Two sub-containers are 
especially important here. 

The first sub-container is an R-language data frame that stores
the geochemical concentrations for each field sample. Each row of this data 
frame pertains to one field sample, and the name of the row is the name of the
field sample. Each column pertains to one element, and the name of the column is the abbreviated name of the element (for example, "Al" or "Zn"). So, a row of 
the data frame comprises all element concentrations for one field 
sample, and a column of the data frame comprises the concentrations of
one element for all field samples. 

There are three requirements for the measured concentrations. First, the concentrations must not have any missing values. Second, concentrations that are left-censored (that is, below the lower limit of determination) must be replaced by an imputed concentrations. Finally, all concentrations must have the same units. Typically these units are "mg/kg", which is equivalent to "ppm".

The second sub-container is an R-language matrix that stores the spatial locations of the field samples. Each row of the matix pertains to one field sample and is associated with a corresponding row of the data frame. The matrix has two columns, which store the coordinates of the field samples. The coordinates are typically lattitude and longitude; the coordinates must have the same datum, which is typically the WGS84 datum.

## Second element
The second list element is called "censorIndicators", and it contains information about the censoring of the measured concentrations. The list element is an R-language array.  The structure of the array is identical to the structure of the data frame containing the concentrations: The array and the data frame have the same number of rows, the same number of columns, the same row names, and the same column names. Consequently, identifying the status of a particular concentration is easy. An array element may have two possible values: "no" meaning that the concentration is not censored and "left" meaning that the original concentration is left-censored and has been replaced by an imputed value.

## Third element
The third list element is called "constSumValue". Geochemical data sum to a constant value even if some element concentrations are not reported. For geochemical data, the constant sum value typically is 1000000 mg/kg or equivalently 1000000 ppm. In such cases, `constSumValue` equals 1000000.

# Preparatory steps

Create a directory named `Process1`, and set the working directory to 
`Process1`. Execute these scripts:
```
gcData <- CoGeochemData
save(gcData, file="gcData.dat")
```
The first script assigns the Colorado geochemical data to variable `gcData`. The format of variable `gcData` is described in section [Organization of the geochemical data]; additional information about the Colorado geochemical data is available under item "CoGeochemData" within the package Help. The second script writes `gcData` to binary file `gcData.dat` in the working directory. This operation is strictly unnecessary because these data are available within package GcClust. Nonetheless, this operation should be performed so that the processing steps within this guide are as close as possible to the steps that you will use when you process your own data.

Create a file named `ProcessScripts.R`; you should copy the R-language scripts from this user guide into this file and then execute them. Such a file is valuable for at least two reasons. First, the file is a record of how the data
are processed; this record ensures that the processing is reproducible. Second,  modifying a processing step is relatively easy in most cases: Only the associated R-language script in the file is editted --- the other scripts remain unchanged.

Copy this code into file `ProcessScript.R` and then execute it. (For the remainder of this guide, these "copy-and-execute" instructions are not stated anymore.)
```
library(GcClust)   
library(sp)        
library(maps)      
library(shinystan) 
load("gcData.dat")   # Load the binary file with the geochemical data
```
This code loads the needed libraries and the geochemical data.

To ensure that the geochemical data pertains to the area of interest, plot the locations of the field samples: 
```
# plot the state as a gray polygon and then add axes 
map('state', regions = 'colorado', fill = TRUE, col = "gray60", border = "white")
map.axes()

# add the sample locations
plot( gcData$concData, add = TRUE, pch = 16, cex = 1/3, col = "black")
```

![Figure 1. Sample locations within the state of Colorado.](figures/CoMap.png)

You will notice that five samples are plotted just outside the northern and eastern borders. The likely cause of this discrepancy is that the datum for the coordinates of the border, which are specified within package maps, differs from the WGS84 datum for the sample locations. This descrepancy is minor and has no effect on the processing.

# Data transformation and analysis

Transform the geochemical data to principal components:
```
# Apply the isometric log-ratio transform and the robust principal components 
# transform. 
transData <- transformGcData(ugDemo$concData)

save(transData, file = "TransData.dat")     
```
The principal components, which are needed for the clustering, are stored in container `transData` along with other variables.

Function `transformGcData` has a second argument `alpha`, which is assigned a default value in the above scripts. Argument `alpha` is the fraction of the data that is used to calculate the mean vector and the covariance matrix; it may range from 0.50 to 1.00. Relative low values, near 0.50, are appropriate for data with high measurement error; whereas relatively high values, near 1.00, are appropriate for data with low measurement error. There is no formula for selecting a value; it is a matter of judgement. Our quality control analyses suggest that the geochemical concentrations in `CoGeochemData`, and hence in `gcData`, have low measurement error. Consequently, `alpha` is set to 0.98, which is its default value. 

Within `gcData` there are concentrations for 39 elements, so after the isometric log-ratio transformation there are 38 components. That is, the number of elements minus 1 equals the number of components. Hence, there are also 38 principal components.

The principal components are now analyzed graphically to check that they are properly computed and to select appropriate parameters for the clustering. 
```
# plot the correlation matrix and histogram of correlations
plotEdaCorr(transData)
```

![Figure 2. (upper) Correlation matrix for the principal components. The red diagonal corresponds to the diagonal of the correlation matrix. (lower) Histogram of the correlations.](figures/EdaCorr.png)

Both the correlation matrix and the histogram (Figure 2)
indicate that the correlations are small; this property is expected for principal components. As argument `alpha` for function `transformGcData` decreases, the correlations tend to increase slightly. 

```
# plot the scree plot
plotEdaVar(transData)
```

![Figure 3. Scree plot.](figures/EdaVar.png)

The scree plot shows variances associated with the principal components. Above each bar is the “cumulative percentage of the total variance.” To understand this quantity, consider the variances for just the first three principal components, namely 1.742, 0.791, and 0.519. The cumulative variances are 1.742, 2.533, and 3.052. These cumulative variances are expressed as percentages of the total variance, which is 4.973. Thus, the cumulative percentages of the total variance are 35.03%, 50.94%, and 61.38%. These cumulative percentages mean that the first component accounts for 35.03% of the total variance, the first and second components for 50.94%, and the first, second, and third components for 61.38%. 

A suitable subset of principal components must be selected for the clustering. Our selection criterion is that the chosen components must account for most of the variance in the data, which is equivalent to most of the information in the data. Thus, the subset always includes the lower-order components, namely component 1, component 2, and so on. The relevant issue is determining the last component in the subset. There is no clear-cut method of resolving this issue, but the cumulative percentage of the total variance is helpful: The higher the cumulative percentage, the greater the amount of information that is used for the clustering. However, if the cummulative percentage is too high, then noise is included in the clustering. Our experience indicates that a suitable percentage ranges roughly from 75% to 95%. For this analysis, we chose a threshold of 96%, which corresponds to 21 principal components. Consequently, variable `nPCS` is set to 21.

```
# set the number of principal components
nPCs <- 21         
```

```
# plot boxplots and violinplots of the principal components
plotEdaDist(transData)   
```

![Figure 4. (upper) Boxplots and (lower) violinplots of the principal components.](figures/EdaDist.png)

The boxplots and the violinplots (Figure 4) show that the distributions for each component are centered at zero and that the spread of the distributions decreases as the component number increases. This behavior is expected for principal components. The distributions for each component are unimodal.

# Sampling the posterior probability density function
The sampling of the posterior probability density function (pdf) is performed with the following code.
```
samplePars <- sampleFmm(transData, nPCs, 
                        nChainsPerCore = 5, nCpuCores = 7)

save(samplePars, file = "SamplePars.dat")
```
Function argument `nCpuCores` specifies that 7 central processing unit (cpu) cores will be requested from the operating system. Of course, the number of processor that you request must be less than or equal to the number on your computer. Function argument `nChainsPerCore` specifies that 5 separate sampling chains will be computed by each core. So, a total of 35 chains are computed. Our experience is that many chains are needed because there may be multiple modes, and that approximately 35 chains is adequate.

The chains are not returned by function `sampleFmm` because they would require too much memory. Instead, the chains are written to the computer disk in the directory specified by argument `procDir`. The file names have the form "RawSamples?-?.dat"; the first and second question marks are replaced by the processor number and the chain number respectively.

The time that function `sampleFmm` needs to complete computations depends upon many factors including the number of chains for each processor and the number of  field samples. When we processed these data, the time was approximately one hour.

# Analysis and processing of the Monte Carlo samples

## Checking parameter traces for each chain

The analysis of the Monte Carlo samples begins by checking the parameter traces for each chain.  The number of model parameters, and hence the number of parameter traces, equals $N^2 + 3N + 1$ where $N$ is the number of principal components. For the example in this vignette, the number of principal components is 21, so the number of parameter traces is 505. Examining 505 parameter traces for 35 chains would be overwhelming, so only a subset of 5 parameter traces is examined for each chain. The 5 model parameters are theta, element 1 of the mean vector for pdf 1, element 1 of the mean vector for pdf 2, element 1 of the standard deviation vector for pdf 1, and element 1 of the standard deviation vector for pdf 2. These 5 parameter traces are plotted 
for all 35 chains with the following code
```
plotSelectedTraces(samplePars)
```

![Figure 5. Traces for (upper) theta, (middle) element 1 of the two mean vectors, and (lower) element 1 of the two standard deviation vectors. These traces pertain to chain 1.](figures/SelectedTraces.png)

Each trace should appear stationary---the trace should consist of random fluctuations having a constant mean and a constant standard deviation. The two traces for element 1 of two mean vectors should not switch with one another. Likewise, the two traces for element 1 of two standard deviation vectors should not switch with one another. If the traces do not satisfy these criteria, then the sampling of the posterior pdf must be modified. The modification are described in the documentation for function `sampleFmm`.

You may notice switching between chains: For example, theta could be 0.42 for chain 1 and 0.58 for chain 2. Associated with this switch in theta, there are correspond switches for the other 4 model parameters. Such switching is common and normal. 

## Selecting the chains

Another component of the analysis is checking point statistics for the model parameters for every chain. Again, for the example in this vignette, there are 505 model parameters, and examining point statistics for these 505 parameters would overwhelming. So, a subset is examined; the subset comprises the 5 model parameters that were discussed in the previous section and natural logarithm of the likelihood. These statistics are plotted with the following code
```
plotPointStats(samplePars)
```

![Figure 6. Point statistics (upper left) for element 1 of the mean vectors for the two pdfs, (upper right) for element 1 of the standard deviation vectors for the two pdfs, (lower left) theta, and (lower right) log-likelihood. A dot represents the median, and the vertical line represents the 95% credible interval. Blue and red represent pdfs 1 and 2, respectively.  intervals](figures/PointStats.png)

Examine the point statistics for the log-likelihood. The median log-likelihoods are either approximately -7420 or -7500. This result suggests that two modes in the posterior pdf were sampled. The log-likelihood indicates how well the model fits the data---higher values indicate a better fit than lower value do. Consequently, any chain except 22 and 26 could be selected for further analysis. For this example, chains 1, 2, 3, and 4 are selected.

The number of selected chains should range between 2 and 4. If there is just 1 chain, then the statistics that are used to check convergence (which is discussed later) are unreliable. Five or more chains provides little additional information. We try to use 4 chains.

Now examine the point statistics for theta. For chains 1, 2, 3, and 4, the median values are approximately 0.56, 0.56, 0.56, and 0.44. This result indicates that chain 4 is switched relative to the other three. This switch is apparent also in the point statistics for element 1 of the mean vectors and element 1 of the standard deviation vectors. 

On rare occasions, the samples for a chain are are poorly representative a mode, and the point statistics differ enormously from corresponding point statistics for other chains. To include the point statistics for all chains in a plot, the range for the vertical axis must be expanded, making it difficult to analyze the statistics. To overcome this difficulty, the spurious point statistics are excluded from the range calculation using argument `excludedChains`.

## Combining the selected chains

In the previous section, we selected chains 1, 2, 3, and 4, and we determined that chain 4 must be switched so that it conforms to the other three chains. A simple way to archive this information is to store it in a common-separated value file:
```
Chain,isSwitched
1,F
2,F
3,F
4,T
```
The file consists of two columns. The first column specifies the number of the chain that is selected, and the second column specifies whether the parameters in the chain are switched. "T" (TRUE) indicates that they are switched; "F" (FALSE) indicates that they are not. 

The following code reads the common-separated value file, switches the chains if needed, and then combines them.
```
selectedChains <- read.csv("SelectedChains.csv", header = TRUE,
                           stringsAsFactors = FALSE)
combinedChains <- combineChains(samplePars, selectedChains)
save(combinedChains, file = "CombinedChains.dat")
```
The returned object `combinedChains` contains the 4 selected chains, using the format specfied by the rstan library.

## Checking convergence

The convergence of the Monte Carlo sampling is evaluated with another package called "stanstan". The following code launches an application in your web browser.
```
launch_shinystan(combinedChains)
```
With this application, you can view various graphical and statistical summaries that indicate how well the sampling converged. Interpretation of these summaries is beyond the scope of the vignette; information on topic is available in Hoffman and Gelman (2013), Gelman and others (2014, p. 281-288) and Stan Development Team (2015). Limited information is available within application shinystan.

## Checking the model

The mean vectors, the standard deviation vectors, and the correlation matrices for the two pdfs in the finite mixture model may be checked by comparing them to equivalent statistics calculated from the principal components. To this end, it is necessary to calculate the conditional probability that a field sample is associated with pdf 1 in the finite mixture model. This calculation is performed with the following code.
```
condProbs1 <- calcCondProbs1(transData, nPCs, combinedChains)
save(condProbs1, file = "CondProbs1.dat")
```
Container `condProbs1` is a matrix containing the Monte Carlo samples of the conditional probabilites and is described further in the package Help. 

These conditional probabilites, along with the principal components, are used to calculate the the mean vectors, the standard deviation vectors, and the correlation matrices for the both pdfs in the finite mixture model. These statistics are called "observed test statistics" (Gelman and others, 2014, p. 145-146) because they are calculated from the observed data. This calculation is performed with the following code.
```
obsTestStats <- calcObsTestStats(transData, nPCS, condProbs1)
save(obsTestStats, file = "ObsTestStats.dat")
```

For Bayesian model checking, the observed test statistics are compared to replicated test statistics (Gelman and others, 2014, p. 143-153), which are derived from the samples of posterior pdf. For this application, the replicated test statistics are just the Monte Carlo samples of the mean vectors, the standard deviation vectors, and the correlation matrices. The comparison is chosen to be graphical. For the vectors, the comparison is performed with the following code.
```
plotModelCheck_MS(combinedChains, obsTestQuantities)
```

![Figure 7. Model check of (upper row) the mean vectors and (lower row) the standard devation vectors. A red dot represents an observed test statistic. A horizontal black line represents the median of associated Monte Carlo samples, and the vertical line represents its 95% credible interval. The numbers along the top of each plot are the p-values.](figures/Check_MS.png)

In Figure 7, the distribution of the Monte Carlo samples for one model parameter (for example, component 1 of the mean vector for pdf 1) is summarized by the median and the 95% credible interval. The associated test statistic should be within the 95% credible interval, and ideally it should be close to the median. That is, the red dot should be within the interval defined by the vertical black line, and ideally it should be close to the horizonal black line. The p-values are explained later in this section.


For the correlation matrices, the comparison between the test statistics is performed with the following code.
```
plotModelCheck_C(combinedChains, obsTestQuantities)
```

![Figure 8. Model check for the correlation matrices. (upper left) Comparison of the observed test statistic (above the red diagonal) to the replicated test statistic (below the red diagonal) for pdf 1. (upper right) Associated p-values for pdf 1. (lower left and lower right) Corresponding graphics for pdf 2.](figures/Check_C.png)

In Figure 8 (upper left), the replicated test statistic for, say, the correlation between components 5 and 2 is the median of the associated Monte Carlo samples. Figure 8 (upper right) shows the associated p-values, which are explained next. The color scale ranges from the smallest p-value to 0.5,
which is the largest possible p-value.

A measure of the discrepancy between the observed test statistic and the replicated test statistic is the posterior predictive p-value (Gelman and others, 2014, p. 146). It is defined as the probability that the replicated test statistic could be more extreme that the observed test statistic. It mathematical terms, $p = Pr( T_r \geq T_o)$ where $T_r$ is the replicated test statistic and $T_o$ is the observed test statistic. When $T_o$ is in the left tail of the distribution for $T_r$, the p-value is close to 1. This situation can confuse the interpretation of the p-value. So, the mathematical definition is modified slightly: $p = Pr( T_r \leq T_o)$ (Gelman and others, 2014, p. 148). Consequently, the calculated p-value is always less than 0.5. and it may be interpreted in the standard way.

For this model check, all p-values (Figures 7 and 8) are moderate to large, indicating that the discrepancy between the test statistics is small. If there are many p-values that are less than approximately 0.1, the model may have difficiencies that require evaluation.

## Transforming back to the simplex

For each pdf in the finite mixture model, the samples of its mean vector, its standard deviation vector, and its correlation matrix are transformed to equivalent quantities in the simplex---namely, samples of the compositional mean vector and the variation matrix. This transformation is performed with the following code.
```
simplexModPar <- backTransform(gcData, nPCs, transData, combinedChains)
save( simplexModPar, file = "SimplexModPar.dat")
```

# Cluster visualization

## Setting the plotting order of the elements

The order in which the elements are plotted is specified by a vector:
```
# order in which the elements are plotted
elementOrder <- c( "Sr", "U", "Y", "Nb", "La", 
                   "Ce", "Th", "Na", "Al", "Ga", 
                   "Be", "K", "Rb", "Ba", "Pb", 
                   "Cu", "Zn", "Mo", "Mg", "Sc", 
                   "Co", "Fe", "V", "Ni", "Cr", 
                   "Ca", "P", "Ti", "Li", "Mn", 
                   "Sn", "As", "Bi", "Cd", "In", 
                   "S",  "Sb", "Tl", "W")
```
The elements are specified by their common chemical abbreviations. Notice that the first letter is capitialized, and that the second letter (if any) is not. Using this vector, you can readily change the element order to enhance visualization of the clusters.

## Plotting the compositional mean

Plot the compositional means for the two pdfs in the finite mixture model:
```
plotCompMeans(simplexModPar, elementOrder)
```

![Figure 9. Compositional means for the two pdfs. Each dot represents the median of the Monte Carlo samples for the associcated chemical element.](figures/CompMeans.png)

The compositional mean is a vector. The Monte Carlo samples of this vector comprise Monte Carlo samples of each vector element. The later must be summarized so that they can be visualized. To this end, the median of the samples for each vector element is computed. The compositional operation of closure is not applied to the medians. 

This plot is useful for at least two reason. First, the compositional means are expressed in terms of concentrations making it relatively easy to interpret them. Second, this plot can be directly compared to any other plot of compositional means. In contrast, plots of standardized compositional means may be compared only in special cases. This type of plot and its restricted comparison is discussed later in this section.

In this plot of the compositional means, the range of the element concentrations is very large. Consequently, it is sometimes difficult to discern how the element concentrations differ between the two pdfs. Moreover, it is desirable to add information to the plot about the distribution of the Monte Carlo samples, but this cannot be done easily because the range of the distribution is very small compared to the vertical axis. Both difficulties are overcome by standardizing the compositional means:
```
simplexStats <- calcSimplexStats(gcData)
save(simplexStats, file = "SimplexStats.dat")
plotStdCompMeans( simplexModPar, simplexStats, gcData, elementOrder)
```
The call to calcSimpleStats calculates the standardization parameters. Explain this.

![Figure 10. Standardized compositional means for the two pdfs. A dot represents the median of the Monte Carlo samples, and vertical line represents the specified credible interval. The hoizontal black line represents the barycenter.](figures/StdCompMeans.png)

Described this

Caution in interpretation: The plot depends stongly upon the set of geochemical concentrations that are used to compute the standardization. Only those plots using the same standardization can be compared. This situation commonly occurs when the standardization is based on the field samples from the entire survey area. 

Plots based on other standardization stats.

## Plotting the variation matrix

## Plotting the clusters

plotProbMap
The symbol colors, symbol sizes, and symbol type have been chosen so that the symbols are easily perceived when they are plotted on a gray background. Be careful when changing these parameters!
The default values work well, so the occasions to change them should be rare.


# Splitting the geochemical data

When do we stop??

# References
Hoffman, M., and Gelman, A., 2013, The no-U-turn sampler-Adaptively setting path lengths in Hamiltonian Monte Carlo, available on line at http://arxiv.org/abs/1111.4246 (last accessed November 2015).

Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., and Rubin, D.B., 2014, Bayesian data analysis (3rd ed.), CRC Press.

Pawlowsky-Glahn, V., Egozcue, J.J., and Tolosana-Delgado, R., 2015, Modeling and analysis of compositional data: John Wiley and Sons, Ltd.

Stan Development Team, 2015, Stan Modeling Language - User’s Guide and Reference Manual, Version 2.8.0, available on line at http://mc-stan.org/ (last accessed October 2015).


---


Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` setion of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
